<div class="row" style="margin-bottom: 0; text-align: left; margin-top: 20px;">
    <div class="col m8">
        <a href="https://github.com/GokulVSD/MonoDAC" id="about-title">
            <img height="55" width="55" src="{{ url_for('static', filename='github.svg') }}" />
            <h1 style="display: inline; padding-left: 10px;">MonoDAC</h1>
        </a>
    </div>
    <div class="col m4"></div>
</div>
<div class="row" style="margin-top: 0; text-align: left;">
    <div class="col m8">
        <h4 style="margin-top: 0;">Monocular Depth using Atrous Convolutions</h4>
        <div class="divider" style="height: 3px; background-color: #F96167;"></div>
    </div>
    <div class="col m4"></div>
</div>
<div class="row" style="margin-top: -15px;">
    <div class="col m12">
        <p style="color: black; text-align: justify; font-family: 'productlight';">
            &emsp;&emsp;&emsp;Generating depth maps, colloquially known as depth estimation, from a single monocular RGB image has long been known to be an ill-posed problem. Traditional depth estimation techniques involve inference from stereo RGB pairs, via depth cues, or through the use of laser based LIDAR sensors, which produce sparse or dense point clouds depending on the size or cost of the sensor. Most modern smartphones contain more than one image sensor; however, utilising these sensors for depth estimation is infeasible as smartphone vendors restrict access to one image sensor at a time. In other cases, the sensors are of varying quality and focal lengths, rendering them inadequate for the purpose of depth inference. Producing depth maps for monocular RGB images is a crucial task due to their use in Depth-of-Field (DoF) image processing, Augmented Reality (AR), and Simultaneous Localisation and Mapping (SLAM).
        </p>
        <p style="color: black; text-align: justify; font-family: 'productlight';">
            &emsp;&emsp;&emsp;To tackle the above problem, we propose a fully convolutional DCNN approach to learning and generating depth maps from single RGB images, utilising Atrous Convolution layers and ASPP for semantic segmentation and feature pooling & extraction in a convolutional neural network, employing an encoder-decoder architecture. We also apply Bicubic upsampling convolutions to further boost depth estimation accuracy, while simplifying previously proposed architectures so as to improve on performance, taking into consideration the computational and accuracy constraints that plague prior efforts.
        </p>
        <p style="color: black; text-align: justify; font-family: 'productlight';">
            &emsp;&emsp;&emsp;We are showcasing the results of our model using a 3D point cloud view, and have trained our model using a subset of NYUv2 dataset that contains RGB and depth map pairs, which were constructed using Kinect sensors in an unsupervised manner.
        </p>
    </div>
</div>

